# 项目

## 芋道

### 项目

- 架构

	-  

- 构建项目芋道后台管理系统

	- centos s9环境

	- 下载git

		- dnf install -y git

	- 下载jdk

		- 先去官网查看所需jdk版本

			- https://gitee.com/zhijiantianya/yudao-cloud.git

		- 下载jdk17到指定文件夹

			- mkdir ~/yudao && cd ~/yudao

			- wget https://download.oracle.com/java/17/archive/jdk-17.0.12_linux-x64_bin.rpm

			- sudo rpm -ivh jdk-17.0.12_linux-x64_bin.rpm

		- 配置环境

			- export JAVA_HOME=/usr/lib/jvm/jdk-17.0.12-oracle-x64
export CLASSPATH=.:${JAVA_HOME}/lib
export PATH=${CLASSPATH}:${JAVA_HOME}/bin:$PATH

		- 重新加载环境

		- 查看java版本

			- java --version出现如图则成功

	- 安装配置maven

		- yum install maven

		- 配置国内源

			- /etc/maven/settings.xml找到<mirrors>所在行,将下面内容添加到此行下面不能被 <-- ... --> 注释包裹

			- <mirror>
    <id>aliyunmaven</id>
    <mirrorOf>central</mirrorOf>
    <name>aliyun maven</name>
    <url>https://maven.aliyun.com/repository/public </url>
</mirror>

	- 下载后端源码

		- mkdir gitcode && cd gitcode

		- git clone https://gitee.com/zhijiantianya/yudao-cloud.git

		- 切换分支，默认分支java8
cd yudao-cloud/
git checkout -b master-jdk17 origin/master-jdk17

			- 查看pom.xml获悉环境版本

- 中间件服务搭建

	- Mysql

		- docker基础

		- 用户信息、业务数据、配置信息

		- 部署

			- docker run -d -p 3306:3306 \
  --restart=unless-stopped \
  --name=yudao_mysql \
  -e MYSQL_ROOT_PASSWORD=123456 \
  -v "/etc/localtime:/etc/localtime" \
  -v yc_mysql:/var/lib/mysql \
  mysql:8.3

		- 导入数据

			- 将主机数据库文件复制到docker

				- docker cp /tmp/ruoyi-vue-pro.sql yudao_mysql:/tmp/ruoyi-vue-pro.sql

			- 使用mysql创建数据库ruoyi-vue-pro
这是项目指定的数据库名在jar包中无法用sed修改

				- create database `ruoyi-vue-pro`

				- 数据库不允许-使用反引号避免语法错误

			- 导入后端项目下sql目录中的ruoyi-vue-pro.sql进行初始化

				- source /tmp/ruoyi-vue-pro.sql

		- 修改后端配置中数据库地址

			- 数据库连接配置在application-local.yaml
默认127.0.0.1:3306 端口，账号root，密码123456

			- 修改其ip地址,因为文件比较多,我们使用shell批量替换

			- 查找所有application-local.yaml文件
并查看其中的数据库配置

				- find ./ -name application-local.yaml -exec grep -l 'jdbc:mysql://127.0.0.1:3306' {} +

			- 更改数据库配置中的ip地址,注意更换为自己的ip地址

				- find ./ -name application-local.yaml -print0 | xargs -0 sed -i 's|jdbc:mysql://127.0.0.1:3306|jdbc:mysql://192.168.206.129:3306|g'

			- 查看更改后的内容

				- find ./ -name application-local.yaml -exec grep 'jdbc:mysql://192.168.206.129:3306' {} +

	- Redis

		- 作用

			- 缓存数据，浏览器或用户数据缓存，存储用户的 JWT Token 或 Session ID，使各个服务能够共享用户登录状态

			-  Redis 的计数器功能实现访问限流，当单位时间内请求次数超标时进行限制或降级处理

		- 部署

			- docker run -d \
  --restart=unless-stopped \
  --name=yudao_redis \
  -v "/etc/localtime:/etc/localtime" \
  -p 6379:6379 \
  redis

		- 修改后端配置中redis地址

			- find ./ -name application-local.yaml -print0 | xargs -0 sed -i 's|host: 127.0.0.1 # 地址|host: 192.168.206.129 # 地址|g'

			- 查看更改后内容
find ./ -name application-local.yaml -exec grep 'host: 192.168.206.129# 地址' {} +

	- Nacos

		- 概述

			- 一个可靠的“注册中心”和“配置中心”，帮助你的各个微服务组件能够有效地发现彼此、协同工作，并能够集中化、动态化地管理应用程序的配置，从而提升整个系统的可伸缩性、可靠性和可维护性

				- 注册中心

					- 在微服务架构中，不同的服务需要相互调用。Nacos 提供了服务注册与发现的功能，允许服务提供者将自己的网络地址注册到 Nacos，服务消费者可以从 Nacos 获取服务提供者的地址列表，从而实现服务之间的动态发现和调用。

				- 配置中心

					- 应用程序的配置信息（例如数据库连接字符串、中间件地址等）可以存储在 Nacos 中

					- 应用程序可以从 Nacos 动态地获取和更新配置，而无需重启服务，实现配置的集中管理和动态更新。

		- 部署

			- docker run -d \
-p 8848:8848 \
-p 9848:9848 \
--restart=unless-stopped \
--name=yudao_nacos \
-e MODE=standalone \
-v "/etc/localtime:/etc/localtime" \
nacos/nacos-server:v2.5.1

				- #webui 8848

				- #-e设置环境变量，以单机模式运行，一般生产环境部署集群

		- 创建dev命名空间

			- 概述

				- 通过创建不同的命名空间，你可以将不同的环境、不同的项目或者不同的团队的配置和服务隔离开来

				- 假设你的项目有以下几个环境：

dev (开发环境)
test (测试环境)
prod (生产环境)

			- 浏览器访问IP：8848/nacos

			-  

			- 微服务需要在其配置文件（例如 application.yaml）中指定 Nacos 服务器的地址和命名空间

		- 修改后端配置中Nacos地址

			- find ./ -name application-local.yaml -print0 | xargs -0 sed -i 's|server-addr: 127.0.0.1:8848|server-addr: 192.168.206.129:8848|g'

- 启动后端服务

	- 中间件服务搭建已经改好了ip地址,现在需要重新构建后端服务,这样服务才能正确使用修改后的配置文件

	- 构建服务

		- # 进入项目根目录
cd /opt/gitdir/yudao-cloud
mvn clean install package '-Dmaven.test.skip=true'

		- 构建服务完成后后端服务目录下会有一个yudao-*.jar文件,使用java -jar xxx.jar启动这个文件来启动后端服务即可,注意要输对路劲

		- 项目构建时长跟网络和机器配置有关

	- 启动主要后端服务

		- 架构图

			-  

		- 启动gateway服务（网关）

			- 概述

				- 这个服务的主要提供API 服务网关，提供用户认证、服务路由、灰度发布、访问日志、异常处理等功能。

				- 之后的后端服务也是一样的方式启动

			- 创建screen会话（可选）

				- 概述

					- 在一个终端窗口创建多个虚拟终端,能让程序在虚拟终端中保持前台执行哪怕你的终端窗口关闭也不受影响nohup？

					- 适用于在后台运行程序或在多个任务之间切换时

				- screen -R gateway

					- 恢复（或创建并连接到）一个名为 "gateway" 的 Screen 会话

				-  按下按键ctrl+a,松开后再按一下d,就能分离screen窗口,回到之前的终端

				- 关闭会话和服务

					- 查找正在运行的 screen 会话的 ID
screen -ls

					- 强制关闭指定的 screen 会话
screen -S ID -X quit

			- 启动服务

				- java -jar yudao-gateway/target/yudao-gateway.jar

			- 查看服务启动结果

				-  

				- 此时在screen会话，按下按键ctrl+a,松开后再按一下d,就能分离screen窗口,回到之前的终端

			- 访问页面端口

				- 成功

		- 启动system服务（功能模块）

			- 概述

				- 此服务主要是实现系统功能的模块，如用户管理、角色权限配置、系统设置等基础模块

			- 创建screen会话

			- 启动服务

				- java -jar yudao-module-system/yudao-module-system-biz/target/yudao-module-system-biz.jar

			- 成功启动

				-  

			- 浏览器访问两个页面

				- 这两个页面一个是实际的服务页面,一个是经过网关的页面

				- http://192.168.163.135:48081/admin-api/system
http://192.168.163.135:48080/admin-api/system

		- 启动infra服务（日志，监控）

			- 概述

				- 此服务与系统的基础设施相关，例如系统日志、任务调度、资源监控、消息队列管理

			- 创建screen会话

			- 启动服务

				- java -jar yudao-module-infra/yudao-module-infra-biz/target/yudao-module-infra-biz.jar

			- 启动成功

				-  

			- 访问两个页面

				- http://192.168.206.129:48080/admin-api/infra/
http://192.168.206.129:48082/admin-api/infra/

				-  

- 启动前端项目

	- 概述

		- 前端项目先启动到windows上,用于测试调整

	- 软件环境

		- git,node.js还有vscode
官网下载即可

		- 设置git环境变量

			- win+r输入sysdm.cpl,在系统变量新增git路径\bin

	- Vue3后台管理项目

		- 在d盘下创建一个gitcode目录

		- 下载项目源码

			- git bash中

			- git clone https://gitee.com/yudaocode/yudao-ui-admin-vue3.git

			- vscode打开整个项目目录

		- 构建项目

			- cd yudao-ui-admin-vue3

			- npm config set registry https://registry.npmjs.org

				- # 设置国内npm源

			- npm install -g pnpm

				- # 安装pnpm

					- 它通过使用硬链接和符号链接来避免重复下载和存储依赖包

			- pnpm config set registry https://registry.npmmirror.com

				- # 设置国内源

			- pnpm install

				- # 安装项目包

		- 运行项目

			- npm run dev

			- 登录报错（后端失联：防火墙或者连接后端的配置或VPN问题）

			- 更改完之后保存文件

			- 能成功的话,说明系统基本功能没有问题了

- 服务容器化

	- 概述

		- 启动服务了,但是启动方式比较麻烦，多个screen虚拟终端,要么就启动多个窗口

		- 将前后端服务都制作成docker镜像,这样启动和管理就会方便很多

	- 制作后端docker镜像

		- 后端有3个服务,gateway,system,infra
分别进入各自的子目录,项目自己提供了Dockerfile文件,不需要我们生成

		- # 制作gateway镜像
cd /opt/gitdir/yudao-cloud/yudao-gateway
docker build -t yudao_gateway .

		- # 制作system镜像
cd /opt/gitdir/yudao-cloud/yudao-module-system/yudao-module-system-biz
docker build -t yudao_system .

		- # 制作infra镜像
cd /opt/gitdir/yudao-cloud/yudao-module-infra/yudao-module-infra-biz
docker build -t yudao_infra .

	- 启动后端容器

		- 概述

			- 在启动之前要先停止之前启动的服务,停止之后再启动docker

			- 服务启动的时候会自动注册地址到nacos中

			- 如果使用docker的网络注册到nacos中的地址就是docker内部地址

			- 如果3个服务在同一个机器上还好,如果在不同的机器上,是无法通过docker内部地址访问的

			- 这里为了方便,我们使用host的网络方式

			- 这样docker使用宿主机的ip地址,注册时也是宿主机的ip地址

			- 之后服务启动到k8s中不会有这个问题,因为k8s使用网络插件可以打通多个节点之间的网络,使用内部ip地址也可以进行通信

		- 停止之前的服务

			- 停止之后再启动docker

			- 关闭会话和服务

				- 查找正在运行的 screen 会话的 ID
screen -ls

				- 强制关闭指定的 screen 会话
screen -S ID -X quit

		- 启动3个后端服务

			- docker run -d \
  --network=host \
  --name yudao_gateway \
  -v "/etc/localtime:/etc/localtime" \
  yudao_gateway

			- docker run -d \
  --network=host \
  --name yudao_system\
  -v "/etc/localtime:/etc/localtime" \
  yudao_system

			- docker run -d \
  --network=host \
  --name yudao_infra\
  -v "/etc/localtime:/etc/localtime" \
  yudao_infra

		- 通过docker logs查看启动成功

			- 参数-f可以实时查看日志的更新,查看完毕后使用ctrl+c来退出查看

			- docker logs -f yudao_gateway 

			- docker logs -f yudao_system

			- dcoker logs -f yudao_infra

	- 制作前端docker镜像

		- 环境

			- linux centos

				- 若构建项目卡住，内存或cpu不够

				- 推荐16g内存

			- linux安装nodejs

				-  

				- 解压到yudao目录下

				- 配置环境变量

					- vim ~/.bashrc

					- export PATH=$PATH:/opt/node-v20.11.1-linux-x64/bin
注意未来更换的版本

				- 重新加载配置

					- source ~/.bashrc

		- 下载前端项目

			- cd ~/yudao

			- git clone https://gitee.com/yudaocode/yudao-ui-admin-vue3.git

		- 配置

			- 修改配置文件.env.local内容中的ip地址

				- cd yudao-ui-admin-vue3

				- vim .env.local

				- loadbalancer绑定的域名

		- 构建项目

			- 下载nodejs

			- npm config set registry https://registry.npmmirror.com
切换源

			- cd /path/to/yudao-ui-admin && npm install

			- 注意内存推荐8g(如果没有其他容器运行否则11g)
npm run build:local

			- 项目会将编译好的前端静态页面存放到当前目录下的dist目录中

		- 制作镜像

			- 新建Dockerfile文件,写入以下内容

				- FROM nginx
ADD ./dist/ /usr/share/nginx/html/

			- 构建镜像

				- docker build -t yudao_ui_admin .

		- 启动容器

			- docker run --name yudao_ui_admin -d -p 8080:80 yudao_ui_admin

			- 访问页面 http://192.168.206.129:8080

- Harbor&&compose

	- 概述

		- harbor

			- 私有的、功能更强大的 Docker Hub

		- docker compose

			- 定义和管理多容器 Docker 应用程序的工具。 通过一个单独的 docker-compose.yml 文件，你可以配置应用程序的所有服务、网络、卷等，然后使用简单的命令一次性启动、停止和管理它们。

	- 下载

		- harbor2.10.2

			- https://github.com/goharbor/harbor/releases

			- online下载解压 ./install.sh

		- docker compose

			- https://github.com/docker/compose/releases

			- linux x86_84

	- 初始化

		- docker-compose2.35

			-  mv docker-compose-linux-x86_64 docker-compose

			- chmod +x docker-compose

			- mv docker-compose /usr/local/bin/

				- 设置环境变量

			- 成功

		- harbor

			- cp harbor.yml.tmpl harbor.yml

				- 复制成可执行文件

			- vim harbor.yml

				- 更改hostname为本机ip
sed -i 's/reg.mydomain.com/192.168.206.129/' harbor.yml

				- 注释https内容
sed -i '13,20 s/^/#/' harbor.yml

			- ./install.sh  在线导入镜像

		- 自动启动

			- vim /etc/systemd/system/harbor.service

			- [Unit]
Description=Harbor Container Registry
Requires=docker.service
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=true
WorkingDirectory=/opt/harbor
ExecStart=/usr/local/bin/docker-compose up -d
ExecStop=/usr/local/bin/docker-compose down

[Install]
WantedBy=multi-user.target

			- daemon-reload
enable --now

	- 测试登录

		- ip:80

		- admin

		- Harbor12345

- 搭建k8s集群

	- 环境

		- ubuntu:22.04，k8s版本为1.29，docker版本为26.0
版本差小也可

		- 3台机器搭建一个1个master节点,2个node节点的集群

			- 修改hosts

		- 运行所有服务资源使用情况，两节点推荐4cpu5g内存

	- 关闭每个节点swap分区

		- 概述

			- 关闭 swap 确保系统只使用实际的物理内存，让 Kubernetes 能够更精确地管理和分配资源，并保证容器的高性能和稳定性。

		- windterm工具有同步输入

		- swapoff -a

		- /etc/fstab中注释掉swap相关挂载

	- 每个节点下载

		- 下载docker

			- /etc/docker/daemon.json

			- {
  "data-root": "/data/docker",
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m",
    "max-file": "100"
  },
  "insecure-registries": ["harbor.xinxainghf.com","192.168.206.129:80"],
    "dns": ["8.8.8.8", "8.8.4.4"],
    "registry-mirrors": [
        "https://docker.m.daocloud.io/",
        "https://huecker.io/",
        "https://dockerhub.timeweb.cloud",
        "https://noohub.ru/",
        "https://dockerproxy.com",
        "https://docker.mirrors.ustc.edu.cn",
        "https://docker.nju.edu.cn",
        "https://xx4bwyg2.mirror.aliyuncs.com",
        "http://f1361db2.m.daocloud.io",
        "https://registry.docker-cn.com",
        "http://hub-mirror.c.163.com"
    ],
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
    }
  }
}

		- 安装cri-dockerd

			- 概述

				- 什么是 CRI

					- Kubernetes 定义的一个标准接口

					- 它允许 Kubernetes 的组件 (kubelet) 与不同的容器运行时进行通信，例如 Docker、containerd 等。

				- 什么是 cri-dockerd

					- 是一个 CRI 兼容的适配器或桥梁

					- 由于 Docker 本身并不直接实现 CRI，cri-dockerd 的作用是让 Kubernetes 可以通过 CRI 来管理 Docker。

					- 一个“翻译器”，将 Kubernetes 的 CRI 请求转换为 Docker 可以理解的命令

			- https://github.com/Mirantis/cri-dockerd/releases 

			- 阅读说明，需要的docker版本

			- 解压复制到/usr/bin

				- tar xvf cri-dockerd-0.3.12.amd64.tgz

				- install -o root -g root -m 0755 ./cri-dockerd/cri-dockerd /usr/bin/cri-dockerd

		- 初始化cri-dockerd

			- 启动文件

				- 创建文件/etc/systemd/system/cri-docker.service

				  
				  [Unit]
				  Description=CRI Interface for Docker Application Container Engine
				  Documentation=https://docs.mirantis.com
				  After=network-online.target firewalld.service docker.service
				  Wants=network-online.target
				  Requires=cri-docker.socket
				  
				  [Service]
				  Type=notify
				  ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
				  ExecReload=/bin/kill -s HUP $MAINPID
				  TimeoutSec=0
				  RestartSec=2
				  Restart=always
				  StartLimitBurst=3
				  StartLimitInterval=60s
				  LimitNOFILE=infinity
				  LimitNPROC=infinity
				  LimitCORE=infinity
				  TasksMax=infinity
				  Delegate=yes
				  KillMode=process
				  
				  [Install]
				  WantedBy=multi-user.target
				  
				  
				- 创建文件/etc/systemd/system/cri-docker.socket

				  
				  [Unit]
				  Description=CRI Docker Socket for the API
				  PartOf=cri-docker.service
				  
				  [Socket]
				  ListenStream=%t/cri-dockerd.sock
				  SocketMode=0660
				  SocketUser=root
				  SocketGroup=docker
				  
				  [Install]
				  WantedBy=sockets.target
				  
				  
			- 启动cri-docker

				- systemctl daemon-reload

				- systemctl enable --now cri-docker.socket

		- 安装对应k8s软件包

			- 概述

				- kubeadm:用于初始化集群

				- kubelet:在集群中每个节点上用来启动pod和容器

				- kubectl:与集群通信的命令行工具

			- 安装软件仓库

				- vim /etc/yum.repo.d/kubenetes.repo

				- [kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg

			- 开始安装

				- yum install -y kubeadm kubectl kubelet

				- *enable kubelet.service*

	- master配置kubeadm

		- 创建文件kubeadm_init.yaml

		  apiVersion: kubeadm.k8s.io/v1beta3
		  bootstrapTokens:
		  - groups:
		    - system:bootstrappers:kubeadm:default-node-token
		    token: abcdef.0123456789abcdef
		    ttl: 24h0m0s
		    usages:
		    - signing
		    - authentication
		  kind: InitConfiguration
		  localAPIEndpoint:
		    advertiseAddress: 192.168.206.129
		    bindPort: 6443
		  nodeRegistration:
		    criSocket: unix:///var/run/cri-dockerd.sock
		    imagePullPolicy: IfNotPresent
		    taints: null
		  ---
		  apiServer:
		    timeoutForControlPlane: 4m0s
		  apiVersion: kubeadm.k8s.io/v1beta3
		  certificatesDir: /etc/kubernetes/pki
		  clusterName: kubernetes
		  controllerManager: {}
		  dns: {}
		  etcd:
		    local:
		      dataDir: /var/lib/etcd
		  imageRepository: registry.aliyuncs.com/google_containers
		  kind: ClusterConfiguration
		  kubernetesVersion: 1.29.1
		  networking:
		    dnsDomain: cluster.local
		    serviceSubnet: 10.96.0.0/12
		    podSubnet: 10.244.0.0/16
		  scheduler: {}
		  
		  
			- k8s版本与配置文件kubernetesVersion一行一致

				- kubectl version

			- api版本警告

				- 使用kubeadm config migrate --old-config k8s/kubeadm_init.yaml --new-config k8s/kubeadm_init_new.yaml更新最新api版本

		- 下载配置文件中的必须镜像

			- 概述

				- 确保所需的 Kubernetes 控制平面和核心组件镜像在初始化之前已经下载到本地

			- kubeadm config images pull --config ./kubeadm_init.yaml

	- 初始化K8S集群

		- kubeadm init --config ./kubeadm_init.yaml

		- 成功后终端最后一排输出包含了 --discovery-token-ca-cert-hash，保存用于node节点连master

			- 用于验证控制平面 CA 证书的哈希值

		- 复制config文件到指定目录

			- mkdir -p $HOME/.kube

			- cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

				- /etc/kubernetes/admin.conf：这是 Kubernetes 集群的管理员配置文件的路径。

				- 这个文件包含了连接到 Kubernetes API Server 所需的凭据和配置信息。

				- 对于 Kubernetes 集群的管理员来说，这个文件非常重要，因为它提供了对集群的完全访问权限。

			- chown root:root ~/.kube/config

		- 删除k8s集群

			- 概述

				- Kubernetes 会在删除 Pod 后根据其控制器（例如 Deployment、StatefulSet、DaemonSet）的定义自动创建一个新的 Pod 实例来替换它

			- 如果你正在使用 Docker (cri-dockerd)

				- sudo kubeadm reset --force --cri-socket unix:///var/run/cri-dockerd.sock

			- 如果你正在使用 containerd

				- sudo kubeadm reset --force --cri-socket unix:///var/run/containerd/containerd.sock

			- 然后根据终端输出删除对应文件

	- 配置集群网络

		- 配置最简单的网络Flannel

			- 加载 bridge

				-  yum install -y epel-release

				-  yum install -y bridge-utils

				-  modprobe br_netfilter

				- echo 'br_netfilter' >> /etc/modules-load.d/bridge.conf
 echo 'net.bridge.bridge-nf-call-iptables=1' >> /etc/sysctl.conf
 echo 'net.bridge.bridge-nf-call-ip6tables=1' >> /etc/sysctl.conf
 echo 'net.ipv4.ip_forward=1' >> /etc/sysctl.conf

				- sysctl -p

			- 概述

				- Flannel 为集群内所有的 Pod 提供统一的扁平网络，使得不同节点上的 Pod 可以直接互相通信，而不必关心底层的网络细节（如物理机、路由、IP 分配等底层细节）

				- 通常以 DaemonSet 形式部署在每个节点上

				- Flannel 依赖内核模块 br_netfilter 来捕获和处理桥接流量

			- kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

			- 下不了去官网下传到服务器

		- 运行命令检查coreDNS pod是否处于Running状态

			- kubectl get pods -A

			- 问题：

				- 分析：查看日志

					- 分析：/proc/sys/net/bridge/bridge-nf-call-iptables 文件只在 Linux 内核加载了 br_netfilter 模块时才会出现。Kubernetes，特别是使用像 Flannel 这样的网络插件时，通常要求加载这个模块，并设置相关的 sysctl 参数（net.bridge.bridge-nf-call-iptables 和 net.bridge.bridge-nf-call-ip6tables 为 1），以确保桥接流量能够被 iptables 处理

				- 解决：加载该模块

					- echo 'br_netfilter' >> /etc/modules-load.d/br_netfilter.conf

					- 加载 br_netfilter 内核模块
sudo modprobe br_netfilter

					- 等待pod或直接删除Flannel Pod ，让 DaemonSet 重新创建自动适配模块：
kubectl delete pod -n kube-flannel <flannel-pod-NAME>

			- 问题

				- 分析：查看describe
  Warning  Failed          41m (x2 over 44m)    kubelet            Failed to pull image "ghcr.io/flannel-io/flannel:v0.26.7": rpc error: code = Canceled desc = context canceled
拉取镜像缓慢

				- 解决：渡渡鸟拉取国内地址
然后tag为ghcr.io/flannel-io/flannel:v0.26.7

	- 两个node节点加入集群

		- 节点开启br_netfilter模块

			- echo 'br_netfilter' >> /etc/modules-load.d/br_netfilter.conf

			- 加载模块
modprobe br_netfilter

		- kubeadm join 192.168.206.129:6443 --cri-socket /var/run/cri-dockerd.sock \
--token abcdef.0123456789abcdef \
--discovery-token-ca-cert-hash sha256:46ba901fffcd09ca181959bf14711827f8bc718bb68da5e4856daa19fd7f603f

			- 192.168.206.129:6443

				- Kubernetes 控制平面 (Master 节点) 的地址和端口在kubeadm_init.yaml指定

			- --cri-socket /var/run/cri-dockerd.sock

				- 告诉 Kubernetes，当前节点上的容器运行时是通过 cri-dockerd 来管理的，需要通过这个 socket 文件来进行通信

			- --token abcdef.0123456789abcdef

				- 加入集群所需的身份验证令牌，kubeadm_init.yaml指定

			- --discovery-token-ca-cert-hash 

				- 指定控制平面 CA 证书的哈希值

			- 使用时注意更改成对应的ip和hash

			- sudo kubeadm token create --print-join-command

				- 生成新的token和ca证书

		- kubectl get nodes

			- 执行命令查看集群中所有节点

			- 成功加入集群，接下来安装组件

- 安装K8S组件

	- helm

		- 概述

			- 是 Kubernetes 的包管理器，类似apt，yum那样的包管理工具

			- Helm 的包被称为 Chart，一个 Chart 包含了描述一组 Kubernetes 资源的所有 YAML 文件

				- 例如，一个 Chart 可以用来部署一个 Web 应用、一个数据库或者一个复杂的微服务应用

			- Release 是 Chart 的一个运行实例，当一个 Chart 被安装到 Kubernetes 集群中时，就会创建一个 Release

		- https://github.com/helm/helm/releases

		- 解压复制到/usr/bin/目录下

			- tar xvf helm-v3.14.2-linux-amd64.tar.gz

			- mv linux-amd64/helm /usr/bin/

	- dashboard

		- 概述

			- 仪表盘,更直观的查看k8s中的资源情况

		1. 7.0之后的版本只能使用helm安装

			- # 添加存储库

				- helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/

			- # 安装或者升级dashboard 

				- helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard

		2. 创建服务账户

			- 新建文件dashboard-adminuser.yaml

			  
			  apiVersion: v1
			  kind: ServiceAccount
			  metadata:
			    name: admin-user
			    namespace: kubernetes-dashboard
			  ---
			  apiVersion: rbac.authorization.k8s.io/v1
			  kind: ClusterRoleBinding
			  metadata:
			    name: admin-user
			  roleRef:
			    apiGroup: rbac.authorization.k8s.io
			    kind: ClusterRole
			    name: cluster-admin
			  subjects:
			  - kind: ServiceAccount
			    name: admin-user
			    namespace: kubernetes-dashboard
			  
			  
		3. 应用到集群

			- kubectl apply -f dashboard-adminuser.yaml

		4. 创建令牌,用于页面登录

			- kubectl -n kubernetes-dashboard create token admin-user

			- 复制输出的内容,这个就是登录要用到的令牌，令牌重启失效

		- 访问dashboard

			- kubectl get svc -A查看dashboard网址

			- https://ip

			- 输入密码kubectl -n kubernetes-dashboard create token admin-user的输出

	- nginx ingress

		- 概述

			- 整个集群的出口,相当于最集群前端的nginx

			- 负责将外部的 HTTP/HTTPS 请求根据域名和路径(yaml文件指定)规则路由到集群内相应的 Service。

		- 下载yaml文件

			- # 下载配置文件

		- 替换其中用到的镜像

			- 浏览器输入https://github.com/kubernetes/ingress-nginx/blob/main/deploy/static/provider/cloud/deploy.yaml
复制内容

				- 需要科学上网

			- sed -i 's$registry.k8s.io/ingress-nginx/controller:v1.10.0.*$registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.10.0$' nginx_ingress.yaml

				- sed -ri 's$registry.k8s.io/ingress-nginx/controller:(v?[0-9]+\.[0-9]+\.[0-9]+([-a-zA-Z0-9.]*)?)$registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:\1$' nginx_ingress.yaml

			- sed -i 's$registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0.*$registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.4.0$' nginx_ingress.yaml

				- sed -ri 's$registry.k8s.io/ingress-nginx/kube-webhook-certgen:(v?[0-9]+\.[0-9]+\.[0-9]+([-a-zA-Z0-9.]*)?)$registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:\1$' nginx_ingress.yaml

			- sed -i 's/@sha.*//' nginx_ingress.yaml

				- 删除镜像摘要

			- 注意版本

		- 应用到集群

			- kubectl apply -f nginx_ingress.yaml

		- 查看pod启动状态

			- kubectl get pod -n ingress-nginx

			- 问题：ImagePullBackOff

				- 分析：查看describe下载镜像失败

		- 2个状态为Completed,1个状态为Running即为成功

	- metallb

		- 概述

			- 为裸机或者非云环境 Kubernetes 集群提供负载均衡（LoadBalancer）功能的组件

			- 在集群中将 Service 类型设置为 LoadBalancer 后，将外部流量路由到内部服务的作用

			- 让服务的LoadBalancer可以绑定局域网ip

		- 使用helm安装

			- # 添加repo源

				- helm repo add metallb https://metallb.github.io/metallb

			- # 安装

				- helm install metallb metallb/metallb

				- 这里没指定namespace，默认default

			- 卸载

				- helm uninstall <release-name> -n <namespace>

		- 创建配置meatllb.yaml
更换IP

		  apiVersion: metallb.io/v1beta1
		  kind: IPAddressPool
		  metadata:
		    name: first-pool
		  spec:
		    addresses:
		    - 192.168.206.200-192.168.206.220
		  ---
		  apiVersion: metallb.io/v1beta1
		  kind: L2Advertisement
		  metadata:
		    name: example
		  
		  
			- 注意IP必须与网关同一个子网
ip r查看网关和子网

		- 应用到集群

			- kubectl apply -f metallb.yaml
修改配置文件重新apply就行

			- 查看kubectl get pod -A

		- 更改网络模式

			- kubectl get svc -A

			- kubectl edit  svc kubernetes-dashboard-kong-proxy  -n kubernetes-dashboard

				- 改为type: LoadBalancer

- k8s创建harbor镜像仓库

	- 信任仓库

		- vim /etc/docker/daemon.json

		- "insecure-registries" : ["192.168.206.129:80(harbor配置地址)"]

		- 重启docker

	- 推送前后端镜像

		- tag

			- docker tag yudao_infra 192.168.163.135:80/library/yudao_infra
docker tag yudao_system 192.168.163.135:80/library/yudao_system
docker tag yudao_gateway 192.168.163.135:80/library/yudao_gateway

		- 上传

			- docker login 192.168.206.129(harbor地址)

			- docker push 192.168.163.135:80/library/yudao_ui_admin
docker push 192.168.163.135:80/library/yudao_infra
docker push 192.168.163.135:80/library/yudao_system
docker push 192.168.163.135:80/library/yudao_gateway

		- 登录harbor网站查看是否上传成功

- K8S中启动项目

	- 后端服务的启动

		- push的私有仓库地址

		- kubectl create deployment yudao-gateway --image=192.168.206.129:80/library/yudao_gateway

			- Deployment是Kubernetes 的一个核心对象，它提供了一种声明式的更新 Pod 和 ReplicaSet 的方法。

		- kubectl create deployment yudao-system --image=192.168.206.129:80/library/yudao_system

		- kubectl create deployment yudao-infray --image=192.168.206.129:80/library/yudao_infra

		- yaml部署

		  # yudao-gateway Deployment
		  apiVersion: apps/v1
		  kind: Deployment
		  metadata:
		    name: yudao-gateway
		  spec:
		    replicas: 1
		    selector:
		      matchLabels:
		        app: yudao-gateway
		    template:
		      metadata:
		        labels:
		          app: yudao-gateway
		      spec:
		        containers:
		          - name: yudao-gateway
		            image: 192.168.163.135/library/yudao_gateway
		            ports:
		              - containerPort: 48080
		  
		  ---
		  # yudao-system Deployment
		  apiVersion: apps/v1
		  kind: Deployment
		  metadata:
		    name: yudao-system
		  spec:
		    replicas: 1
		    selector:
		      matchLabels:
		        app: yudao-system
		    template:
		      metadata:
		        labels:
		          app: yudao-system
		      spec:
		        containers:
		          - name: yudao-system
		            image: 192.168.163.135/library/yudao_system
		            ports:
		              - containerPort: 48081
		  
		  ---
		  # yudao-infra Deployment
		  apiVersion: apps/v1
		  kind: Deployment
		  metadata:
		    name: yudao-infra
		  spec:
		    replicas: 1
		    selector:
		      matchLabels:
		        app: yudao-infra
		    template:
		      metadata:
		        labels:
		          app: yudao-infra
		      spec:
		        containers:
		          - name: yudao-infra
		            image: 192.168.163.135/library/yudao_infra
		            ports:
		              - containerPort: 48082
		  
		  ---
		  # yudao-ui-admin Deployment
		  apiVersion: apps/v1
		  kind: Deployment
		  metadata:
		    name: yudao-ui-admin
		  spec:
		    replicas: 1
		    selector:
		      matchLabels:
		        app: yudao-ui-admin
		    template:
		      metadata:
		        labels:
		          app: yudao-ui-admin
		      spec:
		        containers:
		          - name: yudao-ui-admin
		            image: 192.168.163.135/library/yudao_ui_admin:v1
		            ports:
		              - containerPort: 80
		  
		- Errot和CrashLoopBackOff和imagepulloff

			- 查看错误的后端服务日志

				- can not create connection to database server

					- 分析:可能跟服务器开机自启动docker有关

					- 解决：重启一下对应的数据库

				- can not create connection to registry

					- 分析：集群下载后端服务错误，私有镜像仓库未运行

					- 解决：docker start $(docker ps -a | grep goharbor | cut " " -f 1)
启动goharbor镜像仓库

		- 查看pod的node位置

	- 前端服务的启动

		- 因为之后要使用域名访问,所以前端访问后端服务的地址也要改成域名的方式,需要更改重新构建前端项目

		- 编辑前端项目下的.env.local文件

			- loadbalancer绑定的域名

		- 重新构建镜像并上传

			- npm run build:local

			- docker build -t 192.168.163.135/library/yudao_ui_admin:v1 .

			- docker push 192.168.163.135/library/yudao_ui_admin:v1

		- kubectl create deployment yudao-ui-admin --image=192.168.163.135:80/library/yudao_ui_admin:v1

		- 为2个服务创建SVC

			- svc" 通常指的是 Service，它定义了一种访问 Pod 的方式

			- 需要为gateway服务和前端服务创建一个SVC,方便ingress调用

			- svc_yudao.yaml

			  
			  apiVersion: v1
			  kind: Service
			  metadata:
			    labels:
			      app: yudao-gateway
			    name: yudao-gateway
			  spec:
			    ports:
			    - port: 48080
			      protocol: TCP
			      targetPort: 48080
			    selector:
			      app: yudao-gateway
			    type: ClusterIP
			  ---
			  apiVersion: v1
			  kind: Service
			  metadata:
			    labels:
			      app: yudao-ui-admin
			    name: yudao-ui-admin
			  spec:
			    ports:
			    - port: 80
			      protocol: TCP
			      targetPort: 80
			    selector:
			      app: yudao-ui-admin
			    type: ClusterIP
			  
			  
			- ingress_yudao.yaml

			  
			  apiVersion: networking.k8s.io/v1
			  kind: Ingress
			  metadata:
			    creationTimestamp: null
			    name: yudao
			  spec:
			    ingressClassName: nginx
			    rules:
			    - host: api.ymyw.net
			      http:
			        paths:
			        - backend:
			            service:
			              name: yudao-gateway
			              port:
			                number: 48080
			          path: /
			          pathType: Prefix
			    - host: www.ymyw.net
			      http:
			        paths:
			        - backend:
			            service:
			              name: yudao-ui-admin
			              port:
			                number: 80
			          path: /
			          pathType: Prefix
			  
			  
			- apply

	- 配置域名解析来访问

		- 使用ingress之后必须使用域名这样才能把对应的路径的请求转发到后端服务

		- 查看ingress暴露的ip地址

			- kubectl get svc ingress-nginx-controller -n ingress-nginx
EXTERNAL-IP

		- 将域名解析到EXTERNAL-IP

			- hosts
192.168.163.200 api.ymyw.net
192.168.163.200 www.ymyw.net

			- admin 运行 cmd

			- ipconfig /flushdns

		- 通过非edge浏览器访问域名

			- 关闭本地dns，设置->wlan/以太网->硬件属性->DNS改为手动

			- 关闭VPN，否则劫持本地hosts

- 问题

	- 凡事看日志，日志没问题看下一个相关pod日志，不要以为没日志，有问题一定有日志

		- 数据库取名包含’-‘等冲突字符加` `反引号消除歧义

	- 使用非edge浏览器

	- 登录异常系统错误

		- 分析：后端失联（防火墙或前端连接后端的配置错误）

		- 解决：.env.local

	- ImagePullErr

		- 分析：descibe镜像拉取慢

		- 渡渡鸟或删除pod重新拉取

			- 重新拉取：第一次可能所有号码都占线或无人接听，但第二次拨打时，可能就有一个号码接通了

		- 重启kubelet

	- k8s集群网络

		- Flannel 

			- Flannel status error或未running

				- 分析：查看日志kubectl logs <namespace> <name>发现模块br_netfilter未加载

				- 解决：

					- echo 'br_netfilter' >> /etc/modules-load.d/br_netfilter.conf

					- 加载模块
modprobe br_netfilter

					- 删除Flanpod，让他重建
kubectl delete pod -n kube-flannel <flannel-pod-名称>

			- Flannel 一直Init:ErrImagePull

				- 分析：查看describe -n namespace podname
  Warning  Failed          41m (x2 over 44m)    kubelet            Failed to pull image "ghcr.io/flannel-io/flannel:v0.26.7": rpc error: code = Canceled desc = context canceled
拉取镜像缓慢

				- 解决：渡渡鸟拉取国内地址
然后tag为ghcr.io/flannel-io/flannel:v0.26.7

		- ingress-nginx

			- ingress-nginx未running

				- 集群节点flannel网络问题，同上解决

			- ingress-nginx状态ImagePullBackOff

				- 配置文件image的源问题，换成镜像源，controler一个源，create和patch一个源

		- metallb

			- Pulling image "quay.io/frrouting/frr:9.1.0"

				- 镜像拉取慢

		- can not create sandbox

			- 镜像无法下载，换镜像源或着查看本地镜像仓库是否有目的内容

		- 没有资源访问权限

			- 分析：配置文件问题

			- 解决：官网更换对应的配置文件即可

		- journalctl -u kubelet 

			- 查看警告和错误级别的日志

	- k8s下载错误：为仓库 'kubernetes' 下载元数据失败 : repomd.xml GPG signature verification error: Bad PGP signature

		- 分析：阿里云镜像源内密钥有效期23天，转换unix时间戳比对发现已过期

		- 解决：更换镜像源（cnk8s官网）或将gpgcheck=0

	- win浏览器更新hosts后需更新dns缓存

		- admin 运行 cmd
ipconfig /flushdns

	- ip r没输出网卡默认DOWN

		- nmcli device发现所有网卡未managed

		- 临时

			- sudo ip addr add 192.168.206.131/24 dev ens160

			- ip link set ens160 up

			- ip route add default via 192.168.206.2 dev ens160

			- restart NM

	- masterIP改变

		- oldip=192.168.206.129; newip=192.168.100.129; grep -rl "$oldip" /etc/kubernetes  ~/.kube/conifig 2>/dev/null | xargs sed -i "s#$oldip#$newip#g";

		- 修改kubeadm-config.yaml里的为新ip
kubeadm init phase kubeconfig all --config /etc/kubernetes/kubeadm-config.yaml

			- 为了避免今后换 IP、换主机名时再出问题，可以在 kubeadm 的配置中提前指定：
controlPlaneEndpoint: "k8s-api.example.com:6443"
然后在 DNS 或 /etc/hosts 中映射这个域名到具体 IP。

		- kubeadm init phase certs apiserver --apiserver-advertise-address=NEW_IP

		- mv /etc/kubernetes/manifests/kube-controller-manager.yaml /tmp/
mv /etc/kubernetes/manifests/kube-scheduler.yaml /tmp/
mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/ 
mv /etc/kubernetes/manifests/etcd.yaml /tmp/

		- master重启kubelet，再将yaml移回来，再重启kubelet

	- 命名空间terminating

		- 确认ns状态kubectl get ns kube-flannel

		- kubectl get namespace kube-flannel -o json > /tmp/kube-flannel.json
导出该ns定义

		- 编辑文件删除 Finalizers
打开 /tmp/kube-flannel.json，找到结尾部分
把 "finalizers" 那一整段 删除，或改成空列表：

		- 提交修改，强制清除
kubectl replace --raw "/api/v1/namespaces/kube-flannel/finalize" -f /tmp/kube-flannel.json

			- 它直接调用 Kubernetes API 绕过控制器，手动清除终结标记

		- 或者kubectl get namespace kube-flannel -o json | jq '.spec.finalizers=[]' | kubectl replace --raw "/api/v1/namespaces/kube-flannel/finalize" -f -

	- 修改配置后构建镜像docker会复用前层，只要 COPY target/yudao-infra.jar 的 文件内容校验（hash）没变化，就会用上次的层导致新jar未被更新至镜像

		- 禁用缓存强制构建
docker build --no-cache -t master:80/library/yudao_infra:v1 .

### 面试

- 项目中怎么部署前后端镜像的，为什么这么部署

	- 前后端服务容器化

		- 前后Dockerfile

		- 前先本地构建静态资源，再用 Nginx 镜像打包

	- 推送至 Harbor 私有镜像仓库

		- tag push

	- Kubernetes 部署

		- 使用 kubectl create deployment ... --image=... 或 YAML 文件

		- 前和gateway创建svc方便ingress调用

		- 配置 Ingress 资源（如 ingress_yudao.yaml），实现基于域名的流量分发

			- 基于域可以方便地配置 SSL 证书，安全

			- 不同域名可对应不同的访问策略，安全管理

			- 增加新服务，增加域名和路由规则即可

- MetalLB

	- l2模式

		- 原理

			- 当loadbalancer的service被创建，Metallb 会从你配置的 IP 地址池中分配一个局域网内的可用 IP

			- Metallb 的控制器会监听这些 IP 的 ARP 请求（IPv4）或 NDP 请求（IPv6），并主动响应，把这些 IP“绑定”到运行该 Service 的节点上

		- 优点

			- 配置简单适合没有专业网络设备的裸金属或实验环境

	- BGP

		- BGP Peer建立

			- MetalLB会与您网络中的BGP路由器（通常是边缘路由器或核心交换机）建立BGP邻居关系

		- 路由通告（Route Advertisement）

			- MetalLB会通过BGP协议向其BGP Peer通告Service的外部IP地址

		- 流量转发

			- 当外部流量到达BGP路由器并目的IP是Service的外部IP时，路由器会根据MetalLB通告的路由信息，将流量转发到Service对应的Kubernetes节点上

		- Service转发

			- 量到达Kubernetes节点后，Kube-proxy（或Cilium/Calico等网络插件）会根据Service的ClusterIP和Endpoint信息，将流量负载均衡到Service后面的具体Pod上

		- 高可用

			- 节点故障，metallb停止通告，如果有多个节点承载该Service，MetalLB会继续通告其他健康节点的路由

- Helm在若依部署中带来了哪些便利

	- 简化部署

	- 版本管理与回滚

- ingress-nginx

	- 基于域名可以使用ssl安全

	- 前后端解耦，统一入口，前端不用担心后端的ip变化

- helm

	- Metallb 作为网络基础组件，推荐用 Helm 管理，便于后续升级和参数化配置。

	- Ingress-nginx 由于需要灵活调整和镜像源替换，直接用 YAML 文件部署更方便快捷。

### cicd

- 创建yudaoci组

	- import project

		- 采用url方式

		- https://gitee.com/zhijiantianya/yudao-cloud

- build-frontend:
  stage: build-frontend
  image: node:18-alpine
  script:
    - npm config set registry https://registry.npmmirror.com
    - npm install
    - npm run build:local
    - ls 
  artifacts:
    paths:
      - dist/

- 

dockerize:
  stage: dockerize
  image: docker:24.0.5
  services:
    - name: docker:24.0.5-dind
      command: ["--insecure-registry=192.168.206.129:80"]
  variables:
    DOCKER_HOST: tcp://docker:2375/
    DOCKER_TLS_CERTDIR: ""  
  script:
    - docker login -u $HARBOR_REGISTRY_USER -p $HARBOR_REGISTRY_PASSWD $HARBOR_REGISTRY
    - docker build -t ${HARBOR_REGISTRY}/library/yudao_ui_admin:${VERSION} .
    - docker push ${HARBOR_REGISTRY}/library/yudao_ui_admin:${VERSION}

## mysql集群

### statefulset

- 数据库属于有状态服务，这些服务的每个副本都需要：
独立的数据卷
稳定的网络身份
固定的启动顺序

### secret

- 存储mysql密码和复制用户
复制用户权限在statefulset内command脚本实现

- 可自动注入容器

### 创建ns

- mysql

### apply

- get pod -n mysql发现问题启动失败

- describe找问题

- kubectl logs -f mysql-0 -n mysql查看日志

### mysql容器需要初始化，若/var/lib/mysql下无初始文件则不需要初始化

### 指定sock，sock冲突，脚本

